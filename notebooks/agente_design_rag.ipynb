{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "agente_design_rag.ipynb",
      "authorship_tag": "ABX9TyND4iKEboyXgsxF5m93/rY+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcosrbsantos/agente-posts-ai/blob/main/notebooks/agente_design_rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f2sQ72MxjrR"
      },
      "outputs": [],
      "source": [
        "# 1. Instala Git e clona o repositório\n",
        "!apt-get update -qq && apt-get install -qq git\n",
        "!git clone https://github.com/Marcosrbsantos/agente-posts-ai.git\n",
        "%cd agente-posts-ai\n",
        "\n",
        "# 2. Instala bibliotecas gratuitas\n",
        "!pip install --upgrade pip\n",
        "!pip install diffusers transformers accelerate faiss-cpu sentence-transformers langchain bitsandbytes gradio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "EZc08-3kyhdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"Guia Técnico de Design Gráfico para Posts em Mídias Sociais.pdf\")\n",
        "docs = loader.load()\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(\"Chunks criados:\", len(chunks))\n"
      ],
      "metadata": {
        "id": "4GTBLW-_yjj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "vectorstore = FAISS.from_documents(chunks, embed_model)\n"
      ],
      "metadata": {
        "id": "y5Olzr-iyms9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Carrega Falcon-7B-Instruct em 8-bit\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\", trust_remote_code=True)\n",
        "model     = AutoModelForCausalLM.from_pretrained(\n",
        "    \"tiiuae/falcon-7b-instruct\",\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True\n",
        ")\n",
        "hf_pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
        "llm     = HuggingFacePipeline(pipeline=hf_pipe)\n",
        "\n",
        "# Monta o chain RAG\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\":3})\n",
        "qa        = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n"
      ],
      "metadata": {
        "id": "p108BumUyoBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pergunta = \"Como organizar camadas para um post de Instagram?\"\n",
        "resposta = qa.run(pergunta)\n",
        "print(\"Pergunta:\", pergunta)\n",
        "print(\"Resposta:\", resposta)\n"
      ],
      "metadata": {
        "id": "qcGF_ngmypb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gerar_design(briefing: str):\n",
        "    # 1) extrai spec via RAG\n",
        "    spec = {\n",
        "        \"layers\": qa.run(\"Explique a ordem de camadas para um post.\"),\n",
        "        \"colors\": qa.run(\"Quais esquemas de cores complementares?\"),\n",
        "        \"typography\": qa.run(\"Quais regras tipográficas usar?\")\n",
        "    }\n",
        "    # 2) monta prompt de difusão (exemplo simples)\n",
        "    prompt = f\"{spec['layers']} {spec['colors']} {spec['typography']} {briefing}\"\n",
        "    # 3) gera imagem\n",
        "    from diffusers import StableDiffusionPipeline\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=\"auto\"\n",
        "    ).to(\"cuda\")\n",
        "    img = pipe(prompt, num_inference_steps=25, guidance_scale=7.5).images[0]\n",
        "    return img\n",
        "\n",
        "gr.Interface(\n",
        "    fn=gerar_design,\n",
        "    inputs=\"text\",\n",
        "    outputs=\"image\",\n",
        "    title=\"Agente de Design\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "hZagrY62yqq9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}